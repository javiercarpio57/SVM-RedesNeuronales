library(e1071)
library(caret)
porcentaje<-0.7
datos<-iris
set.seed(123)
corte <- sample(nrow(datos),nrow(datos)*porcentaje)
train<-datos[corte,]
test<-datos[-corte,]
modeloSVM_L<-svm(Species~., data=train, cost=2^5, kernel="linear") #98%
modeloSVM_L<-svm(Species~., data=train, cost=2^-5, kernel="linear") #88%
modeloSVM_L<-svm(Species~., data=train, cost=0.5, kernel="linear")#95%
modeloSVM_R<-svm(Species~., data=train, gamma=2^-5, kernel="radial")
modeloSVM_R<-svm(Species~., data=train, gamma=2^1, kernel="radial")
prediccionL<-predict(modeloSVM_L,newdata=test[,1:4])
prediccionR<-predict(modeloSVM_R,newdata=test[,1:4])
modeloTuneado<-tune.svm(Species~., data=train, cost=c(0.01,0.1,0.5,1,5,10,16,20,32), kernel="linear")
predMejorModelo<-predict(modeloTuneado$best.model,newdata = test[,1:4])
confusionMatrix(test$Species,prediccionL)
confusionMatrix(test$Species,prediccionR)
confusionMatrix(test$Species,predMejorModelo)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
#library(dummy)
library(caret)
library(nnet)
library(RWeka)
install.packages(c("dummy", "neural", "neuralnet", "RWeka"))
library(caret)
#library(dummy)
library(caret)
library(nnet)
library(RWeka)
library(caret)
#library(dummy)
library(caret)
library(nnet)
library(RWeka)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
#library(dummy)
library(caret)
library(nnet)
library(RWeka)
library(neural)
library(dummy)
library(neuralnet)
library(dplyr)
library(corrplot)
library(dplyr)
library(plyr)
library(e1071)
library(rpart)
library("rpart.plot")
library(knitr)
houses <- read.csv("train.csv")
set.seed(123)
datos <-select(houses, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, SalePrice)
datos <- na.omit(datos)
matriz_cor <- cor(datos)
corrplot(matriz_cor)
hist(datos$TotalBsmtSF)
qqnorm(datos$TotalBsmtSF)
qqline(datos$TotalBsmtSF, col='red')
plot(datos$TotalBsmtSF, datos$SalePrice)
hist(datos$X1stFlrSF)
qqnorm(datos$X1stFlrSF)
qqline(datos$X1stFlrSF, col='red')
plot(datos$X1stFlrSF, datos$SalePrice)
hist(datos$GrLivArea)
qqnorm(datos$GrLivArea)
qqline(datos$GrLivArea, col='red')
plot(datos$GrLivArea, datos$SalePrice)
hist(datos$GarageArea)
qqnorm(datos$GarageArea)
qqline(datos$GarageArea, col='red')
plot(datos$GarageArea, datos$SalePrice)
hist(datos$SalePrice)
qqnorm(datos$SalePrice)
qqline(datos$SalePrice, col='red')
cluster <- datos
km <- kmeans(datos, 3)
datos$grupo <- km$cluster
g1<- datos[datos$grupo==1,]
g2<- datos[datos$grupo==2,]
g3<- datos[datos$grupo==3,]
if ((min(g1$SalePrice) > min(g2$SalePrice)) && (min(g1$SalePrice) > min(g3$SalePrice))) {
if (min(g2$SalePrice) > min(g3$SalePrice)) {
a <- c("Caro", "Intermedio", "Bajo")
} else {
a <- c("Caro", "Bajo", "Intermedio")
}
} else if ((min(g1$SalePrice) < min(g2$SalePrice)) && (min(g1$SalePrice) < min(g3$SalePrice))) {
if (min(g2$SalePrice) < min(g3$SalePrice)) {
a <- c("Bajo", "Intermedio", "Caro")
} else {
a <- c("Bajo", "Caro", "Intermedio")
}
} else {
if (min(g2$SalePrice) < min(g3$SalePrice)) {
a <- c("Intermedio", "Bajo", "Caro")
} else {
a <- c("Intermedio", "Caro", "Bajo")
}
}
datos$grupo <- mapvalues(datos$grupo, c(1, 2, 3), a)
porcentaje <- 0.7
datos <- cbind(datos, dummy(datos, verbose = T))
colnames(datos)[8:10] <- a
corte <- sample(nrow(datos), nrow(datos) * porcentaje)
train <- datos[corte, ]
test <- datos[-corte, ]
train$grupo <- as.factor(train$grupo)
test$grupo <- as.factor(test$grupo)
modelo <- nnet(grupo~., data = train[, c(1:5, 7)], size=5, rang=0.000001, decay=5e-4, maxit=500)
prediccion2 <- as.data.frame(predict(modelo, newdata = test[, 1:5]))
columnaMasAlta <- apply(prediccion2, 1, function(x) colnames(prediccion2)[which.max(x)])
test$prediccion2 <- columnaMasAlta
confusionMatrix(as.factor(test$prediccion2), as.factor(test$grupo))
NB <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
nnodos='5'
modeloWeka <- NB(grupo~., data=train[, c(1:5, 7)], control=Weka_control(H=nnodos, N=5000, G=TRUE), options=NULL)
test$prediccionWeka <- predict(modeloWeka, newdata = test[, 1:5])
confusionMatrix(test$prediccionWeka, test$grupo)
num <- which(colnames(train) == "Bajo")
modelo <- glm(Bajo~., data = train[, c(1:5, num)], family = binomial(), maxit = 100)
pred <- predict(modelo, newdata = test[, 1:5], type = "response")
prediccion <- ifelse(pred >= 0.5, 1, 0)
confusionMatrix(as.factor(test$Bajo), as.factor(prediccion))
ggplot(data = test, aes(x = TotalBsmtSF, y = Bajo)) +
geom_point(aes(color = as.factor(Bajo)), shape = 1) +
geom_smooth(method = "glm",
method.args = list(family = "binomial"),
color = "gray20",
se = FALSE) +
theme_bw() +
theme(legend.position = "none")
nbTest = select(test, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
nbTrain = select(train, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
modelo <- naiveBayes(as.factor(nbTrain$Bajo)~., data=nbTrain)
predBayes <- predict(modelo, newdata = test[, 1:5])
confusionMatrix(as.factor(predBayes), as.factor(nbTest$Bajo))
lTest = select(test, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
lTrain = select(train, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
modelo <-lm (Bajo~., data = lTrain)
predMLM <-predict(modelo, newdata = lTest[1:5])
prediccion <- ifelse(predMLM >= 0.5, 1, 0)
confusionMatrix(as.factor(prediccion), as.factor(lTest$Bajo))
ctTrain = select(train, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
ctTest = select(test, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
dt_model<-rpart(ctTrain$Bajo~.,ctTrain,method = "class")
rpart.plot(dt_model)
prediccion <- predict(dt_model, newdata = ctTest[1:5])
columnaMasAlta<-apply(prediccion, 1, function(x) colnames(prediccion)[which.max(x)])
ctTest$prediccion<-columnaMasAlta #Se le añade al grupo de prueba el valor de la predicción
cfm<-table(ctTest$Bajo,ctTest$prediccion)
confusionMatrix(table(ctTest$prediccion, ctTest$Bajo))
cfm
rtTrain = select(train, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, SalePrice, Bajo)
rtTest = select(test, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, SalePrice, Bajo)
dt_model<-rpart(rtTrain$SalePrice~.,rtTrain[1:6],method = "anova")
rpart.plot(dt_model)
prediccion <- predict(dt_model, newdata = rtTest[1:5])
rtTest$prediccion <- prediccion
plot(rtTest$SalePrice, col='blue')
points(prediccion, col='green')
g<-datos[datos$Bajo==1,]
g<-datos[datos$datosIntermedio==1,]
g1<-rtTest[rtTest$prediccion < 172000,]
g1$prediccionClase <- 1
g2<-rtTest[rtTest$prediccion >= 172000,]
g2$prediccionClase <- 0
rtTest <- bind_rows(g1,g2)
cfm<-table(rtTest$Bajo,rtTest$prediccionClase)
confusionMatrix(table(rtTest$prediccionClase, rtTest$Bajo))
resultado <- data.frame("Regresion_Logistica" = 0.9453, "Naive_Bayes" = 0.9157, "Regresion_lineal" = 0.9248, "Arbol_de_clasificacion" = 0.9294, "Arbol_de_regresion" = 0.3599, "Redes N. con nnet" = 0.82, "Redes N. con WEKA" = 0.81)
kable(resultado, caption = "Accuracy por modelo")
knitr::opts_chunk$set(echo = TRUE)
install.packages("RWeka")
library(caret)
#library(dummy)
library(caret)
library(nnet)
library(RWeka)
install.packages("RWeka")
library(caret)
#library(dummy)
library(caret)
library(nnet)
library(RWeka)
install.packages("rJava")
install.packages("RWeka")
modeloTuneado<-tune.svm(Species~., data=train, cost=c(0.01,0.1,0.5,1,5,10,16,20,32), kernel="linear")
library(e1071)
library(caret)
porcentaje<-0.7
datos<-iris
set.seed(123)
corte <- sample(nrow(datos),nrow(datos)*porcentaje)
train<-datos[corte,]
test<-datos[-corte,]
modeloSVM_L<-svm(Species~., data=train, cost=2^5, kernel="linear") #98%
modeloSVM_L<-svm(Species~., data=train, cost=2^-5, kernel="linear") #88%
modeloSVM_L<-svm(Species~., data=train, cost=0.5, kernel="linear")#95%
modeloSVM_R<-svm(Species~., data=train, gamma=2^-5, kernel="radial")
modeloSVM_R<-svm(Species~., data=train, gamma=2^1, kernel="radial")
prediccionL<-predict(modeloSVM_L,newdata=test[,1:4])
prediccionR<-predict(modeloSVM_R,newdata=test[,1:4])
modeloTuneado<-tune.svm(Species~., data=train, cost=c(0.01,0.1,0.5,1,5,10,16,20,32), kernel="linear")
predMejorModelo<-predict(modeloTuneado$best.model,newdata = test[,1:4])
confusionMatrix(test$Species,prediccionL)
confusionMatrix(test$Species,prediccionR)
confusionMatrix(test$Species,predMejorModelo)
modeloTuneado2<-tune.svm(Species~., data=train, cost=c(0.01,0.1,0.5,1,5,10,16,20,32), kernel="radial")
predMejorModelo2<-predict(modeloTuneado2$best.model,newdata = test[,1:4])
confusionMatrix(test$Species,prediccionL)
confusionMatrix(test$Species,predMejorModelo)
confusionMatrix(test$Species,predMejorModelo2)
install.packages("rJava")
install.packages("RWeka")
library(caret)
#library(dummy)
library(caret)
library(nnet)
library(RWeka)
R CMD javareconf
install.packages("rJava")
install.packages("RWeka")
install.packages("rJava")
install.packages("RWeka")
install.packages("rJava")
#install.packages("RWeka")
install.packages("rJava")
#install.packages("RWeka")
Sys.setenv("/usr/lib/jvm/java-11-openjdk-amd64/jre")
Sys.setenv(/usr/lib/jvm/java-11-openjdk-amd64/jre)
Sys.setenv(~/usr/lib/jvm/java-11-openjdk-amd64/jre)
library(caret)
#library(dummy)
library(caret)
library(nnet)
#library(RWeka)
library(neural)
library(dummy)
library(neuralnet)
library(dplyr)
library(corrplot)
library(dplyr)
library(plyr)
library(e1071)
library(rpart)
library("rpart.plot")
library(knitr)
houses <- read.csv("train.csv")
set.seed(123)
datos <-select(houses, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, SalePrice)
datos <- na.omit(datos)
matriz_cor <- cor(datos)
corrplot(matriz_cor)
hist(datos$TotalBsmtSF)
qqnorm(datos$TotalBsmtSF)
qqline(datos$TotalBsmtSF, col='red')
plot(datos$TotalBsmtSF, datos$SalePrice)
cluster <- datos
km <- kmeans(datos, 3)
datos$grupo <- km$cluster
g1<- datos[datos$grupo==1,]
g2<- datos[datos$grupo==2,]
g3<- datos[datos$grupo==3,]
hist(datos$X1stFlrSF)
qqnorm(datos$X1stFlrSF)
qqline(datos$X1stFlrSF, col='red')
plot(datos$X1stFlrSF, datos$SalePrice)
hist(datos$GrLivArea)
qqnorm(datos$GrLivArea)
qqline(datos$GrLivArea, col='red')
plot(datos$GrLivArea, datos$SalePrice)
hist(datos$GarageArea)
qqnorm(datos$GarageArea)
qqline(datos$GarageArea, col='red')
plot(datos$GarageArea, datos$SalePrice)
hist(datos$SalePrice)
qqnorm(datos$SalePrice)
qqline(datos$SalePrice, col='red')
cluster <- datos
km <- kmeans(datos, 3)
datos$grupo <- km$cluster
g1<- datos[datos$grupo==1,]
g2<- datos[datos$grupo==2,]
g3<- datos[datos$grupo==3,]
if ((min(g1$SalePrice) > min(g2$SalePrice)) && (min(g1$SalePrice) > min(g3$SalePrice))) {
if (min(g2$SalePrice) > min(g3$SalePrice)) {
a <- c("Caro", "Intermedio", "Bajo")
} else {
a <- c("Caro", "Bajo", "Intermedio")
}
} else if ((min(g1$SalePrice) < min(g2$SalePrice)) && (min(g1$SalePrice) < min(g3$SalePrice))) {
if (min(g2$SalePrice) < min(g3$SalePrice)) {
a <- c("Bajo", "Intermedio", "Caro")
} else {
a <- c("Bajo", "Caro", "Intermedio")
}
} else {
if (min(g2$SalePrice) < min(g3$SalePrice)) {
a <- c("Intermedio", "Bajo", "Caro")
} else {
a <- c("Intermedio", "Caro", "Bajo")
}
}
datos$grupo <- mapvalues(datos$grupo, c(1, 2, 3), a)
porcentaje <- 0.7
datos <- cbind(datos, dummy(datos, verbose = T))
colnames(datos)[8:10] <- a
corte <- sample(nrow(datos), nrow(datos) * porcentaje)
train <- datos[corte, ]
test <- datos[-corte, ]
train$grupo <- as.factor(train$grupo)
test$grupo <- as.factor(test$grupo)
modelo <- nnet(grupo~., data = train[, c(1:5, 7)], size=5, rang=0.000001, decay=5e-4, maxit=500)
prediccion2 <- as.data.frame(predict(modelo, newdata = test[, 1:5]))
columnaMasAlta <- apply(prediccion2, 1, function(x) colnames(prediccion2)[which.max(x)])
test$prediccion2 <- columnaMasAlta
confusionMatrix(as.factor(test$prediccion2), as.factor(test$grupo))
num <- which(colnames(train) == "Bajo")
modelo <- glm(Bajo~., data = train[, c(1:5, num)], family = binomial(), maxit = 100)
pred <- predict(modelo, newdata = test[, 1:5], type = "response")
prediccion <- ifelse(pred >= 0.5, 1, 0)
confusionMatrix(as.factor(test$Bajo), as.factor(prediccion))
ggplot(data = test, aes(x = TotalBsmtSF, y = Bajo)) +
geom_point(aes(color = as.factor(Bajo)), shape = 1) +
geom_smooth(method = "glm",
method.args = list(family = "binomial"),
color = "gray20",
se = FALSE) +
theme_bw() +
theme(legend.position = "none")
nbTest = select(test, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
nbTrain = select(train, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
modelo <- naiveBayes(as.factor(nbTrain$Bajo)~., data=nbTrain)
predBayes <- predict(modelo, newdata = test[, 1:5])
confusionMatrix(as.factor(predBayes), as.factor(nbTest$Bajo))
lTest = select(test, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
lTrain = select(train, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
modelo <-lm (Bajo~., data = lTrain)
predMLM <-predict(modelo, newdata = lTest[1:5])
prediccion <- ifelse(predMLM >= 0.5, 1, 0)
confusionMatrix(as.factor(prediccion), as.factor(lTest$Bajo))
ctTrain = select(train, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
ctTest = select(test, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, Bajo)
dt_model<-rpart(ctTrain$Bajo~.,ctTrain,method = "class")
rpart.plot(dt_model)
prediccion <- predict(dt_model, newdata = ctTest[1:5])
columnaMasAlta<-apply(prediccion, 1, function(x) colnames(prediccion)[which.max(x)])
ctTest$prediccion<-columnaMasAlta #Se le añade al grupo de prueba el valor de la predicción
cfm<-table(ctTest$Bajo,ctTest$prediccion)
confusionMatrix(table(ctTest$prediccion, ctTest$Bajo))
cfm
rtTrain = select(train, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, SalePrice, Bajo)
rtTest = select(test, TotalBsmtSF, X1stFlrSF, GrLivArea, GarageCars, GarageArea, SalePrice, Bajo)
dt_model<-rpart(rtTrain$SalePrice~.,rtTrain[1:6],method = "anova")
rpart.plot(dt_model)
prediccion <- predict(dt_model, newdata = rtTest[1:5])
rtTest$prediccion <- prediccion
plot(rtTest$SalePrice, col='blue')
points(prediccion, col='green')
g<-datos[datos$Bajo==1,]
g<-datos[datos$datosIntermedio==1,]
g1<-rtTest[rtTest$prediccion < 172000,]
g1$prediccionClase <- 1
g2<-rtTest[rtTest$prediccion >= 172000,]
g2$prediccionClase <- 0
rtTest <- bind_rows(g1,g2)
cfm<-table(rtTest$Bajo,rtTest$prediccionClase)
confusionMatrix(table(rtTest$prediccionClase, rtTest$Bajo))
resultado <- data.frame("Regresion_Logistica" = 0.9453, "Naive_Bayes" = 0.9157, "Regresion_lineal" = 0.9248, "Arbol_de_clasificacion" = 0.9294, "Arbol_de_regresion" = 0.3599, "Redes N. con nnet" = 0.82, "Redes N. con WEKA" = 0.81)
kable(resultado, caption = "Accuracy por modelo")
modeloTuneado<-tune.svm(grupo~., data=train[, c(1:5, 7)], cost=c(0.01,0.1,0.5,1,5,10,16,20,32), kernel="linear")
predMejorModelo<-predict(modeloTuneado$best.model,newdata = test[,1:5])
confusionMatrix(test$grupo,predMejorModelo)
modeloTuneado<-tune.svm(grupo~., data=train[, c(1:5, 7)], cost=c(0.01,0.1,0.5,1,5,10,16,20,32), kernel="linear")
modeloTuneado<-tune.svm(grupo~., data=train[, c(1:5, 7)], cost=c(0.01,0.1,0.5,1,5,10,16,20,32), kernel="radial")
predMejorModelo<-predict(modeloTuneado$best.model,newdata = test[,1:5])
confusionMatrix(test$grupo,predMejorModelo)
modeloTuneado<-tune.svm(grupo~., data=train[, c(1:5, 7)], cost=c(0.01,0.1,0.5,1,5,10,16,20,32), kernel="radial")
predMejorModelo<-predict(modeloTuneado$best.model,newdata = test[,1:5])
confusionMatrix(test$grupo,predMejorModelo)
modeloTuneado<-tune.svm(grupo~., data=train[, c(1:5, 7)], cost=c(0.01,0.1,0.5,1,5,10,16,20,32), kernel="radial")
predMejorModelo<-predict(modeloTuneado$best.model,newdata = test[,1:5])
confusionMatrix(test$grupo,predMejorModelo)
